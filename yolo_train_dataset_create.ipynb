{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b55eb94-4e81-465d-a531-048772049858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing prerequisites\n",
    "import sys\n",
    "import requests\n",
    "import tarfile\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from glob import glob\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "import random\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d689d070-12c4-4447-833b-0e5114ae779b",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2007196-1115-407c-a71c-bf5307926300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def list_files(directory, file_types=['*.pkl']):\n",
    "#     files = []\n",
    "#     for file_type in file_types:\n",
    "#         files.extend(glob(os.path.join(directory, file_type)))\n",
    "#     return files\n",
    "\n",
    "# def list_image_files(directory, file_types=['*.jpg', '*.jpeg', '*.png']):\n",
    "#     return list_files(directory, file_types=file_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b94899a-3f01-483b-a05a-350a6569e3b3",
   "metadata": {},
   "source": [
    "### Create file hierarchy for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c650a510-2bdf-4e83-95c0-eb818e8d5104",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir yolov8/data/train yolov8/data/val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62919ee-9c0e-4f08-93ff-2a5b05cb8681",
   "metadata": {},
   "source": [
    "### Dataset Download & Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38d9e6cb-69a1-4665-a59e-e106e8108cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract dataset \n",
    "# !tar -xvzf train-0.tar.gz\n",
    "# or for entire dataset\n",
    "# !tar -xvzf publaynet.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e3b9657-f2ab-4512-98d2-400f07098272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup base paths\n",
    "data_path = Path(\"publaynet/\")\n",
    "train_data_path = data_path / \"train/\"\n",
    "\n",
    "assert data_path.exists()\n",
    "assert train_data_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acc6db4-8d47-4982-88f0-ee018996459a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying the file was extracted properly\n",
    "\n",
    "# os.chdir(train_data_path)\n",
    "# train_img_names = list_image_files(\".\")\n",
    "# train_img_names = [train_img[2:] for train_img in train_img_names]\n",
    "# print(len(train_img_names), train_img_names[:5])\n",
    "# os.chdir(\"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a0bfddd-c720-4c2a-9c6d-c3a14f9127ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract labels\n",
    "# !tar -xvzf labels.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a59afa53-6fe6-4709-a497-75dc6a66419a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['images', 'annotations', 'categories'])\n"
     ]
    }
   ],
   "source": [
    "# Parse the JSON file and read all the images and labels\n",
    "with open(f'{data_path}/train.json', 'r') as fp:\n",
    "    train_labels = json.load(fp)\n",
    "\n",
    "# with open(f'{data_path}/val.json', 'r') as fp:\n",
    "#     val_labels = json.load(fp)\n",
    "\n",
    "print(train_labels.keys())\n",
    "# print(val_labels.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cff6a6-6261-4671-9653-eb6557a6c015",
   "metadata": {},
   "source": [
    "### Create split from available train images - one time operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb05999-0568-462d-8324-5adab6d4fdd7",
   "metadata": {},
   "source": [
    "Normally, the full dataset has its own split.  Given we're using train-0 (1 of 7) data archives. There are no images from `val.json` in the `train-0` folder, so we're going to create our own train-validation split from the train data.\n",
    "\n",
    "In the future, we could also try to evaluate what the distribution of the classes for each split is, but is overkill for our use-case. Good to have in mind for a production use-case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d642524-513f-4f30-ad3e-97ef31fc5705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed = 42\n",
    "# random.seed(seed)\n",
    "\n",
    "# # Shuffle the list to ensure randomness\n",
    "# random.shuffle(train_img_names)\n",
    "\n",
    "# # Split the list into 80% training and 20% validation\n",
    "# split_index = int(0.8 * len(train_img_names))\n",
    "# train_images = train_img_names[:split_index]\n",
    "# val_images = train_img_names[split_index:]\n",
    "\n",
    "# # Save the splits into .pkl files\n",
    "# with open('train_images.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_images, f)\n",
    "\n",
    "# with open('val_images.pkl', 'wb') as f:\n",
    "#     pickle.dump(val_images, f)\n",
    "\n",
    "# # Print the results to verify\n",
    "# print(\"Training images:\", len(train_images), train_images[:5])\n",
    "# print(\"Validation images:\", len(val_images), val_images[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4336c0-2015-4056-9e07-98b82af85621",
   "metadata": {},
   "source": [
    "#### Load image names so we have consistency through multiple runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90745370-642e-4502-9a2e-f77eaa1c1985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training images: ['PMC4826661_00004.jpg', 'PMC3214698_00011.jpg', 'PMC4573649_00006.jpg', 'PMC4255692_00007.jpg', 'PMC5886901_00001.jpg']\n",
      "Loaded validation images: ['PMC5148792_00003.jpg', 'PMC5812507_00001.jpg', 'PMC3759700_00001.jpg', 'PMC3403926_00000.jpg', 'PMC5005041_00000.jpg']\n"
     ]
    }
   ],
   "source": [
    "# Load image names cell\n",
    "import pickle\n",
    "\n",
    "with open('train_images.pkl', 'rb') as f:\n",
    "    train_images = pickle.load(f)\n",
    "\n",
    "with open('val_images.pkl', 'rb') as f:\n",
    "    val_images = pickle.load(f)\n",
    "\n",
    "print(\"Loaded training images:\", len(train_images), train_images [:5])\n",
    "print(\"Loaded validation images:\", len(val_images), val_images [:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f735c0f4-7081-4ef7-80d9-ac1aca8256ae",
   "metadata": {},
   "source": [
    "#### We have to normalize pixel values before writing bounding boxes to label files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0e6aeb1-c8a4-4450-8397-be375400f6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_center_norm: 0.2734375\n",
      "y_center_norm: 0.390625\n",
      "width_norm: 0.078125\n",
      "height_norm: 0.15625\n",
      "Normalized YOLO label entry: 0 0.2734375 0.390625 0.078125 0.15625\n"
     ]
    }
   ],
   "source": [
    "def normalize_bbox(obj_x, obj_y, obj_w, obj_h, img_w, img_h, verbose=False):\n",
    "    # Step 1: Calculate the center coordinates of the bounding box\n",
    "    x_center = obj_x + obj_w / 2\n",
    "    y_center = obj_y + obj_h / 2\n",
    "    \n",
    "    # Step 2: Normalize the center coordinates and dimensions\n",
    "    x_center_norm = x_center / img_w\n",
    "    y_center_norm = y_center / img_h\n",
    "    width_norm = obj_w / img_w\n",
    "    height_norm = obj_h / img_h\n",
    "\n",
    "    if verbose:\n",
    "        # Print normalized values\n",
    "        print(f\"x_center_norm: {x_center_norm}\")\n",
    "        print(f\"y_center_norm: {y_center_norm}\")\n",
    "        print(f\"width_norm: {width_norm}\")\n",
    "        print(f\"height_norm: {height_norm}\")\n",
    "\n",
    "    return min(x_center_norm, 1), min(y_center_norm, 1), min(width_norm, 1), min(height_norm, 1)\n",
    "\n",
    "# Example label entry (assuming class_id = 0)\n",
    "class_id = 0\n",
    "normalized_bbox = normalize_bbox(obj_x=150, obj_y=200, obj_w=50, obj_h=100, img_w=640, img_h=640, verbose=True)\n",
    "label_entry = f\"{class_id} {normalized_bbox[0]} {normalized_bbox[1]} {normalized_bbox[2]} {normalized_bbox[3]}\"\n",
    "print(\"Normalized YOLO label entry:\", label_entry)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3ea312-a5c2-43b5-8e86-ab5dbd69145f",
   "metadata": {},
   "source": [
    "#### Copy images to COCO-style file structure; Create labels using class_id and normalized (0-1) bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8668654b-99cb-4e57-8bd5-15d92056bc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf yolov8/data/train/*.txt yolov8/data/train/*.jpg\n",
    "!rm -rf yolov8/data/val/*.txt yolov8/data/val/*.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df40357-f5fd-449b-a722-74d22e820d76",
   "metadata": {},
   "source": [
    "**This cell below takes approx 4h for 13GB of image data, despite certain optimizations.** \n",
    "\n",
    "Unfortunately, we must pass through every single train image  object (335,703 train images), and then iterate through all annotations given the image_id. We then save all annotations which have been 'visited' (parsed) into a set based on their id, so that we may skip them in the future. This approach was chosed due to how the COCO dataset convention works, where we have an \"image\" object(s) dictionary, and multiple annotations for any given image - but they are in separate JSON sub-objects (sub-dictionaries), in no particular order, with no ability of indexing given a certain id or filename. As such, the search is linear through all sub-objects, yielding complexity of O(M * N), where M is the number of images, and N is the number of annotations. Storing parsed annotations simply reduces the number of operations performed, but will not change complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a51404a1-7733-4133-81ad-e69aa8b6f51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "335703it [4:07:23, 22.62it/s]  \n"
     ]
    }
   ],
   "source": [
    "train_img_names_set = set(train_images)\n",
    "val_img_names_set = set(val_images)\n",
    "output_data_path = Path(\"yolov8/data/\")\n",
    "visited_entries_ann = set()\n",
    "\n",
    "for idx, img_obj in tqdm(enumerate(train_labels['images'])):\n",
    "\n",
    "    file_name = img_obj['file_name']\n",
    "    if file_name not in train_img_names_set.union(val_img_names_set):\n",
    "        continue\n",
    "\n",
    "    full_img_path = train_data_path / file_name\n",
    "    \n",
    "    if file_name in train_img_names_set:\n",
    "        img_output_path = output_data_path / \"train\" / file_name\n",
    "        label_output_path = output_data_path / \"train\" / str(file_name[:-3] + \"txt\")\n",
    "    else:\n",
    "        img_output_path = output_data_path / \"val\" / file_name\n",
    "        label_output_path = output_data_path / \"val\" / str(file_name[:-3] + \"txt\")\n",
    "\n",
    "    shutil.copy(full_img_path, img_output_path)\n",
    "    \n",
    "    img_width = img_obj['width']\n",
    "    img_height = img_obj['height']\n",
    "    image_id = img_obj['id']\n",
    "    label = None\n",
    "    \n",
    "    for ann_obj in train_labels['annotations']:\n",
    "        if ann_obj['image_id'] != image_id or ann_obj['id'] in visited_entries_ann:\n",
    "            continue\n",
    "\n",
    "        visited_entries_ann.add(ann_obj['id'])\n",
    "        class_id = int(ann_obj['category_id']) - 1\n",
    "        bbox = ann_obj['bbox']\n",
    "\n",
    "        normalized_bbox = normalize_bbox(*bbox, img_width, img_height)\n",
    "        # print(normalized_bbox)\n",
    "        label_entry = f'{class_id} ' + f'{normalized_bbox[0]} {normalized_bbox[1]} {normalized_bbox[2]} {normalized_bbox[3]}\\n'\n",
    "\n",
    "        # Write the content to the file\n",
    "        with open(label_output_path, 'a') as file:\n",
    "            file.write(label_entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df079d1a-7b97-4590-9841-3399edb5e138",
   "metadata": {},
   "source": [
    "We should now have:\n",
    "- Created the appropriate file structure expected by YOLO, taking the COCO conventions into account  (file hierarchy + files + `custom.yaml`)\n",
    "- All names of train/validation images in `train_images.pkl` and `val_images.pkl` - for reproducibility.\n",
    "- After running the above cell, we should have all images in place, alongside their labels. We are now ready for fine-tuning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
